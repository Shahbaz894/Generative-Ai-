{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs4JqgPFI2KIdPWTaSQ4Ch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahbaz894/Generative-Ai-/blob/main/LangChainTextSplitter_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BijCyBfE2fLn",
        "outputId": "37908de6-9b25-4be5-d1ea-798da2e79ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.47)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain transformers sentence-transformers faiss-cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1️⃣ RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "E--_j-x-tFIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text = \"\"\"Artificial Intelligence (AI) is a rapidly evolving field that encompasses various techniques and methodologies.\n",
        "From natural language processing (NLP) to computer vision, AI enables machines to mimic human-like abilities.\n",
        "Machine learning, a subset of AI, allows computers to learn from data patterns and improve over time.\n",
        "Deep learning, a more advanced subset, uses neural networks to process vast amounts of information.\n",
        "Applications of AI can be found in healthcare, finance, autonomous vehicles, and beyond.\n",
        "\"\"\"\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "phOToTfytEKH",
        "outputId": "47ec0c94-e43c-4bcd-a1af-3e248285d1ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial Intelligence (AI) is a rapidly evolving field that encompasses various techniques and', 'techniques and methodologies.', 'From natural language processing (NLP) to computer vision, AI enables machines to mimic human-like', 'to mimic human-like abilities.', 'Machine learning, a subset of AI, allows computers to learn from data patterns and improve over', 'and improve over time.', 'Deep learning, a more advanced subset, uses neural networks to process vast amounts of information.', 'Applications of AI can be found in healthcare, finance, autonomous vehicles, and beyond.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2️⃣ CharacterTextSplitter\n",
        "\n",
        "📌 Best for: Splitting text based on a specific character (e.g., \\n, .)."
      ],
      "metadata": {
        "id": "r2CYxwhPtYG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text = \"\"\"The internet has revolutionized communication and knowledge sharing.\n",
        "Social media platforms, blogs, and online forums have given everyone a voice.\n",
        "The rapid spread of misinformation, however, remains a significant challenge.\n",
        "With advancements in AI, detecting fake news is becoming more effective.\n",
        "\"\"\"\n",
        "\n",
        "splitter = CharacterTextSplitter(separator=\".\", chunk_size=50, chunk_overlap=10)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "V3R9wLcUtcHR",
        "outputId": "fb4ef41b-9ef6-4a45-d9c8-2daa5cc5d261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 67, which is longer than the specified 50\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 78, which is longer than the specified 50\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 77, which is longer than the specified 50\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 72, which is longer than the specified 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The internet has revolutionized communication and knowledge sharing', 'Social media platforms, blogs, and online forums have given everyone a voice', 'The rapid spread of misinformation, however, remains a significant challenge', 'With advancements in AI, detecting fake news is becoming more effective']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3️⃣ TokenTextSplitter\n",
        "\n",
        "📌 Best for: Splitting based on token count instead of characters.\n",
        "\n",
        "🔹 Uses OpenAI's tiktoken for precise tokenization."
      ],
      "metadata": {
        "id": "Fnipyc2Xto6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "id": "omFDCNSix4eL",
        "outputId": "141b0932-3b90-4ce4-a889-28a4698abf55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "# Example text to split\n",
        "document_text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming the world rapidly. It is being used in various industries such as healthcare, finance, and education.\n",
        "AI models, including deep learning and natural language processing (NLP), are improving decision-making, automation, and efficiency.\n",
        "\n",
        "Machine Learning (ML), a subset of AI, allows systems to learn and improve from experience without being explicitly programmed.\n",
        "Popular ML algorithms include decision trees, support vector machines, and neural networks.\n",
        "\n",
        "Deep Learning, which mimics the human brain's neural networks, has led to advancements in image recognition, speech processing, and generative AI.\n",
        "Large Language Models (LLMs) such as GPT, Mistral, and LLaMA are being trained on vast datasets to generate human-like text and assist in content creation.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize TokenTextSplitter\n",
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size=2000,  # Controls the size of each chunk\n",
        "    chunk_overlap=20,  # Controls overlap between chunks\n",
        ")\n",
        "\n",
        "# Split the document text into smaller chunks\n",
        "text_chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "# Print the first few chunks\n",
        "for i, chunk in enumerate(text_chunks[:3]):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5lEZKSYXttJD",
        "outputId": "2b13ed95-6a2a-4062-8046-1e986cb3cabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "\n",
            "Artificial Intelligence (AI) is transforming the world rapidly. It is being used in various industries such as healthcare, finance, and education. \n",
            "AI models, including deep learning and natural language processing (NLP), are improving decision-making, automation, and efficiency. \n",
            "\n",
            "Machine Learning (ML), a subset of AI, allows systems to learn and improve from experience without being explicitly programmed. \n",
            "Popular ML algorithms include decision trees, support vector machines, and neural networks. \n",
            "\n",
            "Deep Learning, which mimics the human brain's neural networks, has led to advancements in image recognition, speech processing, and generative AI. \n",
            "Large Language Models (LLMs) such as GPT, Mistral, and LLaMA are being trained on vast datasets to generate human-like text and assist in content creation.\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4️⃣ NLTKTextSplitter\n",
        "\n",
        "📌 Best for: Splitting text using sentence tokenization from NLTK.\n",
        "\n",
        "🔹 Code Example"
      ],
      "metadata": {
        "id": "3zp1UnvMuNL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "kWAeJ0RDuc4P",
        "outputId": "a971338e-9429-4a58-9dd2-17b06641786a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "\n",
        "text = \"This is the first sentence. Here is another one. And yet another.\"\n",
        "\n",
        "splitter = NLTKTextSplitter()\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "L5zJoq4ruMFZ",
        "outputId": "d1de1f0e-02b2-4024-a9a7-01cc5d2d5706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.\\n\\nHere is another one.\\n\\nAnd yet another.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5️⃣ SpacyTextSplitter\n",
        "\n",
        "📌 Best for: Splitting text into sentences using SpaCy's NLP capabilities."
      ],
      "metadata": {
        "id": "6Z0CVfG6upGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "text = \"Hello world! This is an AI-powered document processing tool.\"\n",
        "\n",
        "splitter = SpacyTextSplitter()\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "aEXLZjO3uooI",
        "outputId": "54db3029-00c5-4bd8-e406-e56b76ebd346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello world!\\n\\nThis is an AI-powered document processing tool.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6️⃣ MarkdownTextSplitter\n",
        "\n",
        " Best for: Splitting Markdown documents into structured chunks."
      ],
      "metadata": {
        "id": "A4Mi-5BMu5A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "markdown_content = \"\"\"\n",
        "# Introduction\n",
        "AI is reshaping the world in many ways.\n",
        "## History of AI\n",
        "The origins of AI date back to the 1950s.\n",
        "## Applications\n",
        "AI is used in healthcare, finance, and education.\n",
        "\"\"\"\n",
        "\n",
        "splitter = MarkdownTextSplitter()\n",
        "chunks = splitter.split_text(markdown_content)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "79Xr7sy0u17G",
        "outputId": "f5132123-cca3-4b46-bf4a-67b486e25624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# Introduction\\nAI is reshaping the world in many ways.\\n## History of AI\\nThe origins of AI date back to the 1950s.\\n## Applications\\nAI is used in healthcare, finance, and education.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7️⃣ LatexTextSplitter\n",
        "\n",
        "📌 Best for: Splitting LaTeX documents into logical sections."
      ],
      "metadata": {
        "id": "kXhdxC2avNJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import LatexTextSplitter\n",
        "\n",
        "text = \"\\\\section{Introduction} This is the intro. \\\\subsection{Details} More details here.\"\n",
        "\n",
        "splitter = LatexTextSplitter()\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "TG2-AQWTvJHC",
        "outputId": "9fee34f7-2a61-40ba-a41f-40d5a437a352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\\\section{Introduction} This is the intro. \\\\subsection{Details} More details here.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import PythonCodeTextSplitter\n",
        "\n",
        "code = \"\"\"\n",
        "def hello():\n",
        "    print('Hello, World!')\n",
        "\n",
        "class Greeting:\n",
        "    def say_hello(self):\n",
        "        return 'Hello!'\n",
        "\"\"\"\n",
        "\n",
        "splitter = PythonCodeTextSplitter()\n",
        "chunks = splitter.split_text(code)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "llfqqHfSvS41",
        "outputId": "745e17d9-b506-4b2f-b1c0-f67d8c82dcfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"def hello():\\n    print('Hello, World!')\\n\\nclass Greeting:\\n    def say_hello(self):\\n        return 'Hello!'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9️⃣ HTMLHeaderTextSplitter\n",
        "\n",
        "📌 Best for: Splitting HTML documents based on headers."
      ],
      "metadata": {
        "id": "b3su-G7Tvg83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import HTMLSectionSplitter\n",
        "\n",
        "html_content = \"\"\"\n",
        "<h1>Artificial Intelligence</h1>\n",
        "<p>AI is transforming industries worldwide.</p>\n",
        "<h2>Machine Learning</h2>\n",
        "<p>ML is a subset of AI that enables systems to learn.</p>\n",
        "<h2>Deep Learning</h2>\n",
        "<p>Deep learning utilizes neural networks for complex tasks.</p>\n",
        "\"\"\"\n",
        "\n",
        "# Define headers to split on\n",
        "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
        "\n",
        "# Initialize the splitter with headers\n",
        "splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Split HTML content\n",
        "chunks = splitter.split_text(html_content)\n",
        "\n",
        "# Print the chunks\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"🔹 Chunk {i}:\\n{chunk}\\n\")\n"
      ],
      "metadata": {
        "id": "r6PBfzFOvid7",
        "outputId": "3f5341b4-ce8e-4261-9f8d-efe779619974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Chunk 1:\n",
            "page_content='Artificial Intelligence \n",
            " AI is transforming industries worldwide.' metadata={'Header 1': 'Artificial Intelligence'}\n",
            "\n",
            "🔹 Chunk 2:\n",
            "page_content='Machine Learning \n",
            " ML is a subset of AI that enables systems to learn.' metadata={'Header 2': 'Machine Learning'}\n",
            "\n",
            "🔹 Chunk 3:\n",
            "page_content='Deep Learning \n",
            " Deep learning utilizes neural networks for complex tasks.' metadata={'Header 2': 'Deep Learning'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔟 JSONSplitter\n",
        "\n",
        "📌 Best for: Splitting JSON files into logical chunks."
      ],
      "metadata": {
        "id": "sAtt4rnWvwLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_text_splitters import RecursiveJsonSplitter\n",
        "\n",
        "json_data = \"\"\"\n",
        "{\n",
        "  \"name\": \"AI Assistant\",\n",
        "  \"description\": \"An AI system designed to assist users in various tasks.\",\n",
        "  \"capabilities\": [\n",
        "    \"Natural Language Processing\",\n",
        "    \"Machine Learning\",\n",
        "    \"Computer Vision\"\n",
        "  ],\n",
        "  \"applications\": {\n",
        "    \"Healthcare\": \"AI is used for diagnosis and treatment recommendations.\",\n",
        "    \"Finance\": \"AI helps in fraud detection and automated trading.\"\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Convert JSON string to Python dictionary\n",
        "json_dict = json.loads(json_data)\n",
        "\n",
        "# Initialize splitter\n",
        "splitter = RecursiveJsonSplitter(max_chunk_size=150)\n",
        "\n",
        "# Split the JSON dictionary (not string)\n",
        "chunks = splitter.split_json(json_dict)\n",
        "\n",
        "# Print the chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\\n\")\n"
      ],
      "metadata": {
        "id": "FoCIEu_tvv2k",
        "outputId": "74bd3e6a-fed5-47b0-bc32-681904cdf424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: {'name': 'AI Assistant', 'description': 'An AI system designed to assist users in various tasks.'}\n",
            "\n",
            "Chunk 2: {'capabilities': ['Natural Language Processing', 'Machine Learning', 'Computer Vision']}\n",
            "\n",
            "Chunk 3: {'applications': {'Healthcare': 'AI is used for diagnosis and treatment recommendations.'}}\n",
            "\n",
            "Chunk 4: {'applications': {'Finance': 'AI helps in fraud detection and automated trading.'}}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Sample long XML data\n",
        "xml_data = \"\"\"\n",
        "<articles>\n",
        "    <article>\n",
        "        <title>AI in Healthcare</title>\n",
        "        <content>Artificial Intelligence (AI) is revolutionizing healthcare by improving diagnosis, treatment, and patient care. AI-powered systems analyze medical data to detect diseases like cancer early, helping doctors make informed decisions. Machine learning models are also used in drug discovery, predicting potential compounds and speeding up research. However, challenges like data privacy and ethical concerns remain critical issues.</content>\n",
        "    </article>\n",
        "    <article>\n",
        "        <title>Advancements in Quantum Computing</title>\n",
        "        <content>Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers. Companies like Google and IBM are racing to build more powerful quantum processors. These computers could revolutionize cryptography, materials science, and artificial intelligence. Despite the progress, practical quantum computers are still in their infancy, requiring further breakthroughs in qubit stability and error correction.</content>\n",
        "    </article>\n",
        "    <article>\n",
        "        <title>Future of Space Exploration</title>\n",
        "        <content>Space exploration is entering a new era with ambitious missions from NASA, SpaceX, and other organizations. Mars colonization, asteroid mining, and deep-space travel are becoming real possibilities. Advanced propulsion systems, AI-driven navigation, and robotic missions are making space more accessible. Challenges such as radiation exposure, long-duration space travel, and planetary sustainability must be addressed to ensure successful missions.</content>\n",
        "    </article>\n",
        "</articles>\n",
        "\"\"\"\n",
        "\n",
        "# Parse XML data using BeautifulSoup\n",
        "soup = BeautifulSoup(xml_data, \"xml\")\n",
        "\n",
        "# Extract text content from XML\n",
        "articles = soup.find_all(\"article\")\n",
        "text_chunks = [article.get_text(separator=\" \") for article in articles]\n",
        "\n",
        "# Initialize LangChain text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
        "\n",
        "# Split extracted text into chunks\n",
        "chunks = splitter.split_text(\" \".join(text_chunks))\n",
        "\n",
        "# Print the chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "It80OVc4vso2",
        "outputId": "17f194f9-f40b-413b-f915-0370adc20095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "AI in Healthcare\n",
            "\n",
            "Chunk 2:\n",
            "Artificial Intelligence (AI) is revolutionizing healthcare by improving diagnosis, treatment, and patient care. AI-powered systems analyze medical data to detect diseases like cancer early, helping\n",
            "\n",
            "Chunk 3:\n",
            "like cancer early, helping doctors make informed decisions. Machine learning models are also used in drug discovery, predicting potential compounds and speeding up research. However, challenges like\n",
            "\n",
            "Chunk 4:\n",
            "However, challenges like data privacy and ethical concerns remain critical issues.\n",
            "\n",
            "Chunk 5:\n",
            "Advancements in Quantum Computing\n",
            "\n",
            "Chunk 6:\n",
            "Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers. Companies like Google and IBM are racing to build more powerful\n",
            "\n",
            "Chunk 7:\n",
            "racing to build more powerful quantum processors. These computers could revolutionize cryptography, materials science, and artificial intelligence. Despite the progress, practical quantum computers\n",
            "\n",
            "Chunk 8:\n",
            "practical quantum computers are still in their infancy, requiring further breakthroughs in qubit stability and error correction.\n",
            "\n",
            "Chunk 9:\n",
            "Future of Space Exploration\n",
            "\n",
            "Chunk 10:\n",
            "Space exploration is entering a new era with ambitious missions from NASA, SpaceX, and other organizations. Mars colonization, asteroid mining, and deep-space travel are becoming real possibilities.\n",
            "\n",
            "Chunk 11:\n",
            "becoming real possibilities. Advanced propulsion systems, AI-driven navigation, and robotic missions are making space more accessible. Challenges such as radiation exposure, long-duration space\n",
            "\n",
            "Chunk 12:\n",
            "exposure, long-duration space travel, and planetary sustainability must be addressed to ensure successful missions.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}